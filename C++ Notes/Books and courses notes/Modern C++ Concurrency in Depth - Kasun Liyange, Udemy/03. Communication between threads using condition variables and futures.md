## Conditional variables
`std::condition_variable` is a synchronization primitive used with a `std::mutex` to block one or more threads until another thread both modifies a shared variable (the _condition_) and notifies the `std::condition_variable`.

A condition variable is associated with some event, and one or more threads can wait for that event to happen. If some thread has determined that the event is satisfied, it can then notify one or all threads waiting for that condition variable to wake them up and allow them to continue processing.

After calling the member function `{cpp}void wait(std::unique_lock<std::mutex>& lock, Predicate pred);`, the thread will be blocked until a notification is received from another thread or after _spurious (random) awakenings from the OS_. The predicate `pred` is optional, but this predicate can be used to ignore spurious awakenings - if the predicate is in the `false` state, the thread will fall asleep until the next awakening. Right after `{cpp}wait()` returns, the mutex inside `std::unique_lock` is locked by the calling thread.

Besides `{cpp}wait()`, there also exists `{cpp}wait_for()`, which in addition takes a timeout, and `{cpp}wait_until()`, which takes a time point (similar to `{cpp}std::this_thread::sleep_for()`/`{cpp}std::this_thread::sleep_until()`). Those functions also might have an optional predicate, but the overload with a predicate has a different return value:
- without predicate: functions return one of the values from `std::cv_status`, either `std::cv_status::timeout` or `std::cv_status::no_timeout`.
- with predicate: `bool` with the last predicate result.

The thread that intends to modify the shared variable must acquire `std::mutex` (typically via `std::lock_guard`) before modifying the shared variable and call `{cpp}notify_one()` or `{cpp}notify_all()` after finishing the modification (can be done after releasing the lock).
```cpp
std::mutex m;
std::condition_variable cv;
int currentStop = 0;

void bus() {
    for (int stop = 1; stop <= 3; ++stop) {
        std::this_thread::sleep_for(std::chrono::milliseconds(500));
        
        {
            std::lock_guard lg{m};
            currentStop = stop;
            std::println("Bus at stop {}", stop);
        }
        cv.notify_all();
    }
}

void passenger() {
    std::unique_lock ul{m};
    cv.wait(ul, [] { return currentStop == 2; });
    std::println("Passenger exits at stop 2");
}

int main() {
    std::jthread p{passenger};
    std::jthread b{bus};
}
```

### Thread safe queue implementation
> Section more based on book C++ Concurrency in Action, 2nd Edition by A Williams, since looks like author on this course most of example took from that book

`std::queue`, similar to `std::stack`, is not thread-safe; therefore, some adaptation is required. Similar to a [[02. Thread safe access to shared data and locking mechanisms#Thread safe stack implementation.|thread-safe implementation of a stack]], we are going to wrap `std::queue` function calls with locking mechanisms. We also need to adapt the interface to avoid race conditions. Consequently, we need to combine `front()` and `pop()` into a single function call.
The receiving thread often needs to wait for the data. Let’s provide two variants for taking an element from the queue:
1. `{cpp}try_pop()`: Tries to pop the value from the queue but always returns immediately (with an indication of failure) even if there wasn’t a value to retrieve.
2. `{cpp}wait_and_pop()`: Waits until there’s a value to retrieve.
    
For the `{cpp}wait_and_pop()` function, we are going to use a condition variable. If the underlying `std::queue` is empty, the condition variable will block all threads requesting data. We will also add a predicate to ignore random OS thread awakenings (spurious wakeups), since in this function we guarantee that it will return a value. Inside `{cpp}push()`, we will call a notify function on the condition variable.
> Note, I used `std::optional` instead `std::shared_ptr` since 

We could use `notify_all()` without worrying about race conditions, since only one thread will obtain the lock over the mutex. The thread that "wins" the mutex race will check its predicate. If the predicate is `true`, that thread continues execution with the locked mutex; otherwise, it returns to the wait state.

However, with `notify_all()`, _every_ waiting thread will be woken up, and every thread will check the predicate. If 10 threads are waiting on the condition variable, 1 thread will take the value, and the 9 others will find the queue empty and fall back asleep (the "thundering herd" problem). Therefore, it is usually a wiser decision to notify only one thread. `notify_one()` will wake an arbitrary thread, which is functionally equivalent to a random thread locking the shared mutex, but without spending resources on redundant queue size checks in all other threads.

```cpp
template<typename T>
class threadsafe_queue
{
    mutable std::mutex m_mutex; // 'mutable' allows locking in const member functions (like copy ctor)
    std::queue<T> m_queue;
    std::condition_variable m_condvar;

public: 
    threadsafe_queue() = default;
    
    threadsafe_queue(const threadsafe_queue& other) {
        std::scoped_lock locker(other.m_mutex);
        m_queue = other.m_queue;
    }

    void push(T new_value) {
        std::lock_guard locker{m_mutex};
        m_queue.push(std::move(new_value));
        m_condvar.notify_one();
    }

    std::shared_ptr<T> wait_and_pop() {
        std::unique_lock locker{m_mutex};

        m_condvar.wait(locker, [this]{ return !m_queue.empty(); });
        
        auto element{std::make_shared<T>(data_queue.front())};
        m_queue.pop();

        return element;
    }

    std::shared_ptr<T> try_pop() {
        std::lock_guard locker{m_mutex};
        
        if (m_queue.empty())
            return std::nullopt;
            
        auto element{std::make_shared<T>(data_queue.front())};
        m_queue.pop();

        return element;
    }

    bool empty() const {
        std::lock_guard locker{m_mutex};
        return m_queue.empty();
    }
};
```


## Futures
The STL provides facilities to obtain values returned by _asynchronous tasks_ or to catch exceptions thrown by them. These values are communicated via a **shared state**, where the asynchronous task writes its result or exception. This state can be examined, waited for, and manipulated by threads holding instances of `std::future` or `std::shared_future`.

A _synchronous_ operation blocks the process until completion. An _asynchronous_ operation is non-blocking and only initiates the task; the caller discovers completion via a specific mechanism.

### std::future
`std::future` is a handle used to read information stored in a shared state. It provides a mechanism to access the result of an asynchronous operation.

When a thread needs the result, it calls the `{cpp}get()` member function. If the asynchronous operation has not finished, the calling thread blocks until the result is ready. Alternatively, `{cpp}wait()` can be used to block without retrieving the value.
To avoid indefinite blocking, `{cpp}wait_for()` or `{cpp}wait_until()` can be used. These return a `std::future_status`:
- `std::future_status::deferred`: The function has not started yet.
- `std::future_status::ready`: The result is available.   
- `std::future_status::timeout`: The timeout elapsed before the result was ready.
`{cpp}get()` can only be called once on a `std::future`, any following attempts can rise an exception.

```cpp
#include <iostream>
#include <future>
#include <random>

int asyncFunc() {
    thread_local std::mt19937 rEng{[] {
        std::random_device rd;
        return std::mt19937{rd()};
    }()};
    std::uniform_int_distribution idist(1, 2);
    return idist(rEng);
}

int main() {
    std::future<int> future = std::async(asyncFunc);

    std::cout << "[1]heads or [2]tails?: ";
    int userChoice {0};
    std::cin >> userChoice;
    const int res = future.get();

    std::cout << (res == userChoice ? "You guessed right\n" :
                                      "You guessed wrong\n");
}
```

### std::async
`std::async` is a template function that runs a callable object asynchronously and returns a `std::future` holding the eventual result.
It accepts an optional `std::launch` policy (bitmask):
- `std::launch::async`: The task is executed _on a different thread_.
- `std::launch::deferred`: The task is executed _on the calling thread_ synchronously, but only when `{cpp}get()` or `{cpp}wait()` is called.
If no policy is specified, the implementation acts as `std::launch::async | std::launch::deferred` (system decides).

Deferred Execution Nuances:
1. A deferred task executes in the thread that requests the result.
```cpp
    auto f1 = std::async(std::launch::deferred, task);
    auto f2 = std::async(std::launch::deferred, task);
    
    std::jthread t([&f1]() {
        f1.get();  // task() will be executed in separate thread
    });
    
    f2.get();  // task() will be executed in main thread
```
2. The task executes only once. Any following calls to `{cpp}get()` return the stored result.
3. If `{cpp}get()` or `{cpp}wait()` is never called, the task is never executed.

### Parallel accumulate algorithm implementation with std::async task
In this example we going to change previously discussed [[01. Thread management guide#Parallel accumulate algorithm using std threads|parallel accumulate algorithm using std::threads]] but with `std::async`. 

Similarly, we add constant limit for minimum elements for starting parallel execution to avoid overhead for relatively small arrays which can be execution with one main thread. Also, instead splitting on chunks, we will call function recursively dividing input iterator range on half if number of elements in iterator range is greater than `min_block_size`.  First half we going to calculate in current thread (it can be main thread or separate thread) and for another half - create new `std::async`, after that make a summary between both half. 
```cpp
static constexpr auto min_block_size = 1000;

template<typename Iterator, typename T>
T parallel_accumulate(Iterator begin, Iterator end, T init) {
    const auto inputSize = std::distance(begin, end);

    if (inputSize == 0)
        return init;

    if (inputSize < min_block_size)
        return std::accumulate(begin, end, init);

    Iterator mid = begin;
    std::advance(mid, (inputSize + 1) / 2);

    std::future<T> future = std::async(parallel_accumulate<Iterator, T>, mid, end, init);
    auto sum = parallel_accumulate(begin, mid, init);
    return sum + future.get();
}
```

### std::package_task
`std::packaged_task` wraps a callable object so it can be invoked asynchronously. Its return value (or exception) is stored in a shared state accessed via `std::future`.

Unlike `std::async`, `std::packaged_task` *does not launch a thread*. It is an _invocable_ object (functor) that must be called manually or passed to a `std::thread`. This provides low-level control over _when_ and _where_ the task runs.

Characteristics:
- **Type Erasure:** Like `std::function`, it hides the specific type of the callable.
- **Move-Only:** It cannot be copied, only moved (requires `std::move` when passing to threads).
- **Reusable:** To reuse a `packaged_task`, call `reset()` to discard the previous shared state and create a new one. Note, that `{cpp}operator()` return `void`, since result of execution will be store in `std::future`, which we also should get again after calling `{cpp}reset()`.  

```cpp
int add(const float x, const int y) {
    std::cout << "Called from thread " << std::this_thread::get_id() << '\n';
    return static_cast<int>(x) + y;
}

void task() {
    std::packaged_task<int(float, int)> pt(add);
    std::cout << "Main thread " << std::this_thread::get_id() << '\n';
    
    // First execution (same thread)
    std::future<int> future1 = pt.get_future();
    pt(0.0, 6);
    std::cout << "Result: " << future1.get() << '\n';
    
    // Reuse (separate thread)
    pt.reset();
    std::future<int> future2 = pt.get_future();
    std::thread thread(std::move(pt), 5.0, 6);
    thread.join();
    std::cout << "Result: " << future2.get() << '\n';
}
```

### std::promise 
`std::promise` class provides a facility to store a value or an exception that is later acquired asynchronously via a `std::future` object created by the `std::promise` object. `std::promise` object is meant to be used only once.  

Each `std::promise` associated with a shared state, and `std::promise` can do three things with the shared state: 
+ _make ready_: set some result (with member function`{cpp}void set_value(T&)`/`{cpp}void set_value_at_thread_exit(T&)`) or exception (with member function `{cpp}void set_exception(std::exception_ptr p)`/`{cpp}void set_exception_at_thread_exit(std::exception_ptr p)`),  marks the state ready and unblocks any thread waiting on a future;
+ _release_: the promise gives up its reference to the shared state. If this was the last such reference, the shared state is destroyed. This happen when `std::promise` moved to another `std::promise` (shared state remain the same) or when `std::promise` destroyed after setting value/exception; 
+ _abandon_: if destructor called before setting any value or exception, stores the exception of type `std::future_error` with error code `std::future_errc::broken_promise`, makes the shared state ready, and then releases it;

`std::promise` and `std::future` can be viewed as a one-directional communication channel. 
 ```
┌─────────────────┐         ┌───────────────────────┐
│  std::promise   │────────▶│   Shared State        │
└─────────────────┘         │  ┌─────────────────┐  │
                            │  │ Result/Exception│  │
┌─────────────────┐         │  │ Ready flag      │  │
│  std::future    │────────▶│  │ Mutex/condition │  │
└─────────────────┘         │  │ Reference count │  │
                            │  └─────────────────┘  │
                            └───────────────────────┘
```

```cpp
void printVal(std::future<int> f) {
    using namespace std::chrono;
    std::println("Waiting for value");
    auto startTime = high_resolution_clock::now();

    auto val = f.get();

    auto duration = duration_cast<milliseconds>(high_resolution_clock::now() - startTime);
    std::println("Value received after {}, value: {}", duration, val);
}

int main() {
    std::promise<int> prom;
    std::jthread t1(printVal, prom.get_future());
    std::this_thread::sleep_for(std::chrono::milliseconds(103));
    prom.set_value(1);
}
```

When we set in promise exception, that exception will be raised in associated `std::future` `{cpp}get()` function. Thus, to "send" an exception from one thread to another, we use in one thread which hold `std::promise` exception with `{cpp}std::set_exception()` and in another thread which holds `std::future` wrap `{cpp}get()` with try-catch block. 

`std::promise` and `std::future` also can use `void` type, this will create _a single-shot_ mechanism. In this case shared state size can be smaller since compilers can use for `<void>` special template instantiation with optimization. 

In this `printVal` firstly will print `Future error caught: std::future_error: Broken promise`, since we destroyed `std::promise` without setting any value (abandon case), and after that `Logical error caught: Not correct value`. Note that `std::future_error` is subclass of `std::logical_error`, in swap catch blocks in `{cpp}printVal()` function - both output will be `Logical error caught`.
```cpp
void printVal(std::future<void> f) {
    try {
        f.get();
    }
    catch (std::future_error& e) {
        std::println("Future error caught: {}", e.what());
    }
    catch (std::logic_error& e) {
        std::println("Logical error caught: {}", e.what());
    }
    catch (...) {
        std::println("Unknown exception caught");
    }

}

int main() {
    {
        std::promise<void> prom;
        std::thread t1(printVal, prom.get_future());
        t1.detach();
    }
    {
        std::promise<void> prom;
        std::thread t1(printVal, prom.get_future());
        prom.set_exception(std::make_exception_ptr(std::logic_error("Not correct value")));
        t1.join();
    }
}
```

### std::shared_future
`std::future` can be get value only once. That mean that if we have more than one threads, they can't share same `std::future`, since after first thread call `{cpp}get()` function, another thread will get an exception after that. Using `{cpp}valid()` member function of `std::future` also not a solution, since it can introduce race condition: both threads can get `true` from `{cpp}valid()` function, when only one thread get value from `{cpp}get()`, so result is depends on order of execution. 

When it's require to use future with multiple threads should be used `std::shared_future`.  Unlike `std::future`, which is only moveable (so only one instance can refer to any particular asynchronous result), `std::shared_future` is copyable and multiple shared future objects may refer to the same shared state. Access to the same shared state from multiple threads is safe if each thread does it through its own copy of a `std::shared_future` object.  `std::shared_future` can be constructed with another copy of `std::shared_future`, by taken ownership of another `std::shared_future` or by taking ownership over `std::future` (`{cpp}shared_future(std::future<T>&& other) noexcept;`). 
```cpp
void printVal(std::shared_future<int> f) {
    std::println("Waiting for value");

    auto tid = std::this_thread::get_id();
    std::println("[{}] Received value: {}", tid, f.get());
}

int main() {
    std::promise<int> prom;
    std::shared_future<int> future {prom.get_future()};

    std::jthread t1(printVal, future);
    std::jthread t2(printVal, future);

    std::this_thread::sleep_for(std::chrono::milliseconds(103));
    prom.set_value(1);
}  
```