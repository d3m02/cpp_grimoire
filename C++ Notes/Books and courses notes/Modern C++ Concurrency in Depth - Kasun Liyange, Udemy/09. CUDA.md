> This course section to be honest, really general 'get-started' information. 

**CUDA (Compute  Unified Device Architecture)** is a parallel computing platform and API for performing _GPGPU_ (General Purpose Computing in GPU) on NVIDIA hardware. 

CUDA requires special compiler (nvcc) to compile source code which uses CUDA, compiler can be obtained from [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)

For linux cuda can be installed from package manager, it also can require to change some environment variables 
```bash
CUDA_PATH=/opt/cuda
PATH="$PATH:/opt/cuda/bin"
NVCC_CCBIN='/usr/bin/g++'
```
and then as simple CMakeLists can be used
```cmake 
cmake_minimum_required(VERSION 4.2)
project(Cuda CUDA)

set(CMAKE_CUDA_STANDARD 23)

add_executable(Cuda main.cu)

set_target_properties(Cuda PROPERTIES CUDA_SEPARABLE_COMPILATION ON)
```

Functions which execute on the GPU which can be invoked from the host are called _kernels_. 

Basic steps of CUDA program are 
+ Initialization of data with CPU  
+ Transfer data from CPU context to GPU context 
+ Kernel launch with needed grid/block size 
+ Transfer results from GPU context back to CPU context
+ Reclaim the memory from both CPU and GPU

CUDA program consist of _Host code_ - sequential code that is run in CPU and _Device code_ - code that is run in GPU.
Kernel function is specified using the `__global__` declaration specifier and should return `void`. 

To launch kernel code we use "Triple chevron notation" (`<<< >>>`) in which provide execution configuration, inside which we provide _grid_ and _thread block_ dimensions. Kernel code asynchronous, meaning that execution in host code will continue without waiting for kernel code execution finish. To wait for finalizing kernel code we can use `{cpp}cudaDeviceSynchronize()`.

To use all that, we also need to include header `<cuda_runtime.h>`. 
```cpp
#include <cuda_runtime.h>
#include <stdio.h>

__global__ void kernel_main() 
{
    printf("CUDA kernal code\n");
}

int main()
{
    kernel_main<<<1, 1>>>();
    cudaDeviceSynchronize();
    
    cudaDeviceReset();
    return 0;
}
```

*Grids* are a collection of all the threads launching for a kernel. Threads in a grid are organized in the *tread blocks*. Thread blocks and grids may be 1, 2, or 3 dimensional. These dimensions can simplify mapping of individual threads to units of work or data items. In example above we used single value to specify one dimensional grid and block, do specify 2D or 3D grid/block we can use type `{cpp}dim3(unsigned int x = 1, unsigned int y = 1, unsigned int z = 1)` (for both 2D and 3D). To specify value layout like this 
```
    (Z)
   /
  +---------------------- (X)
  |   ____  ____  ____
  |  |tttt||tttt||tttt] 
  |  |tttt||tttt||tttt]
  |   ----  ----  ----
  |  ^----------------- 
  |  (grid)      ^----- 
 (Y)             (block) 
```
We use kernel launch parameters like 
```cpp
dim3 grid(3, 1, 1);  // or grid(3)
dim3 block(4, 2, 1); // or block(4, 2)
kernel_main<<<grid, block>>>();
```
Thread block have limitation $(x \le 1024, y \le 1024, z \le 64) \&\& (x * y * z \le 1024)$. For grid theoretical limitations are $x \le 2^{31-1},  y \le 65535, z \le 65535)$, but actual depend on hardware limitations. 

For each thread CUDA implicitly initialize variables. For example `dim3 threadIdx`, which represents thread coordinate inside grid.
```
            |T1 T2 T3|  |T4 T5 T6| < 2 blocks with 3 threads in block
threadIdx.x  0  1  2     0  1  2
threadIdx.y  0  0  0     0  0  0
threadIdx.z  0  0  0     0  0  0
```
Similarly to `dim3 blockIdx`.

```cpp
__global__ void kernal_main() {  
    printf("T<%d>[%d, %d] ",blockIdx.x, threadIdx.x, threadIdx.y);  
}  
  
int main()  
{  
    dim3 grid(2);  
    dim3 block(3, 2);  
    kernal_main<<<grid, block>>>();
    cudaDeviceSynchronize();
    // Output: T<0>[0, 0] T<0>[1, 0] T<0>[2, 0] T<0>[0, 1] T<0>[1, 1] T<0>[2, 1] T<1>[0, 0] T<1>[1, 0] T<1>[2, 0] T<1>[0, 1] T<1>[1, 1] T<1>[2, 1] 
}
```

`dim3 blockDim` variable consist number of threads in each dimension of a thread block. All the thread block in a grid have same block size for all threads in a a grid. 
`dim3 gridDim` variable consist number of thread blocks in each dimension of a grid. 
In general, those two values - same as values used in kernel launcher, but accessible in each thread. 
> [!note] 
> Funny, that overload of comparison `dim3` doesn't exist, and to compare implicit initialized value - it's require to add 
> ```cpp
> inline bool operator==(const dim3& d1, const dim3& d2) {
 >   return (d1.x == d2.x) && (d1.y == d2.y) && (d1.z == d2.z); 
>}
>```

Those variables useful to calculate a indices when inside thread want to make some calculation on shared array.     
 ```cpp
#include <cuda_runtime.h>
#include <cstdio>cudaError
#include <array>
#include <thrust/device_vector.h>

__global__ void kernal_main(int* input, int n) {
    /* Layout is
     *  -------
     * | T0 T1 |
     * | T2 T3 |
     *  -------
     *  -------
     * | T4 T5 |
     * | T6 T7 |
     *  -------
     */
    // 2D coordinates to 1D coordinates: Y * width + X
    auto tid = blockIdx.y * blockDim.y + threadIdx.y    // Global Y coordinate offset
               * gridDim.x * blockDim.x                 // width
               + blockIdx.x * blockDim.x + threadIdx.x; // Global X coordinate
    if (tid < n)
        printf("T[%d]: %d\n", tid, input[tid]);
}

int main()
{
    std::array arr {10, 20, 30, 40, 50, 60, 70, 80};

    // Copy to GPU vector
    thrust::device_vector<int> dev_vec(arr.begin(), arr.end());
    int* raw_ptr = thrust::raw_pointer_cast(dev_vec.data());

    dim3 grid(1, 2);
    dim3 block(2, 2);

    kernal_main<<<grid, block>>>(raw_ptr, arr.size());

    cudaDeviceSynchronize();
    return 0;
}
 ```

Import part is transferring data between host context and device context. For this purpose can be used 
`{cpp} cudaMemcpy(void* dst, const void* src, size_t count, enum cudaMemcpyKind kind)`, where `cudaMemcpyKind` is used to describe direction: 
+ `cudaMemcpyHostToHost` - Host -> Host
+ `cudaMemcpyHostToDevice` -  Host -> Device 
+ `cudaMemcpyDeviceToHost` - Device -> Host
+ `cudaMemcpyDeviceToDevice` - Device -> Device
+ `cudaMemcpyDefault` - Direction of the transfer is inferred from the pointer values. Requires unified virtual addressing

CUDA also have similar to C-style memory function to manage memory allocation: 

| C                                                  | CUDA                                                               |
| -------------------------------------------------- | ------------------------------------------------------------------ |
| `{cpp}void* malloc (size_t size)`                  | `{cpp}cudaError cudaMalloc(void** devPtr, size_t size)`            |
| `{cpp}void* memset (void* src, int val, size_t n)` | `{cpp}cudaError cudaMemset(void* devPtr, int value, size_t count)` |
| `{cpp}void free(void *ptr)`                        | `{cpp}cudaError cudaFree(void* devPtr)`                            |

> In course it's not mentioned, but CUDA also allow to pass `std::array` to kernel code by value. In this case containers "read-only". Above I used another found example of memory management, [thrust::device_vector](https://nvidia.github.io/cccl/thrust/api/classthrust_1_1device__vector.html) from CUDA C++ Core library. 
```cpp
template<size_t N>
__global__ void gpu_sum(std::array<int, N> arrLeft, std::array<int, N> arrRight, int* resArr)
{
    auto tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < N)
        resArr[tid] = arrLeft[tid] + arrRight[tid];
}

int main()
{
    std::array arrA {10, 20, 30, 40, 50, 60, 70, 80};
    std::array arrB {1, 2, 3, 4, 5, 6, 7, 8};
    constexpr auto arrSize = arrA.size();
    constexpr auto arrMemSize = arrSize * sizeof(int);

    int* devPtr;
    cudaMalloc(&devPtr, arrMemSize);

    dim3 grid(1);
    dim3 block(arrSize);
    gpu_sum<<<grid, block>>>(arrA, arrB, devPtr);
    cudaDeviceSynchronize();

    std::array<int, arrSize> arrRes {};
    cudaMemcpy(arrRes.data(), devPtr, arrMemSize, cudaMemcpyDeviceToHost);
    
    std::ranges::for_each(arrRes, [](auto val){ printf("%d ", val);});
    
    cudaFree(devPtr);
    return 0;
}
```

Since during work with CUDA we actually work with two devices (CPU and GPU), error handling a little bit special. First, most of CUDA synchronous functions return [cudaError enum](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038), enum can be used together with `{cpp}const char* cudaGetErrorString(cudaError_t error)`, `{cpp}â€‹const char* cudaGetErrorName (cudaError_t error)`. For asynchronous function can be used `{cpp}cudaError cudaPeekAtLastError()`(which doesn't reset last error code) and `{cpp}cudaGetLastError cudaPeekAtLastError()` (which will reset error code). 

```cpp
    auto res = cudaMemcpy(arrRes.data(), devPtr, -arrMemSize, cudaMemcpyDeviceToHost);
    if (res != cudaSuccess) {
        std::string_view resStr = cudaGetErrorString(res);
        std::cout<< "CUDA memory alloc failed: " << resStr << '\n';
    }
```
