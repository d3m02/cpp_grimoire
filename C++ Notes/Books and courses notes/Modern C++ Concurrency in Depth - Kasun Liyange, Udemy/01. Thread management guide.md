A **Process** is an executing instance of a computer program. When an application is launched, its instructions and data are loaded from persistent storage (disk) into RAM. The OS allocates memory and system resources, and the CPU begins executing the instructions. Each process runs in its own _isolated memory space_. Even with a single core CPU, an operating system can execute multiple applications "simultaneously".

This capability is managed via **Concurrency**. The CPU does not execute all processes strictly simultaneously; instead, it creates the _illusion of parallelism_. The _Scheduler_ (part of the OS) decides which process runs at any given time, assigning time slots (quanta) based on priority and readiness. To switch between processes, the CPU performs a _Context Switch_: it saves the state of the currently running process and restores the saved state of the next one.

**Context** is a collection of data about a process that allows the processor to suspend execution and restart it later exactly where it left off. It includes the program counter, CPU register states, and memory management information.

A **Thread** is the smallest sequence of programmed instructions that can be managed independently by the scheduler. A thread is a component of a process; every process has at least one thread (the _main thread_) which is the entry point for the program. Unlike processes, which run in separate memory spaces, threads within the same process _share the same memory space_ (heap, code, global data), allowing for efficient communication, though they maintain their own stack and registers.

Modern CPUs have multiple cores, allowing multiple threads or processes to execute at the exact same physical moment. This is **Parallelism**. Note that context switching still occurs on multi-core systems if the number of active threads exceeds the number of available hardware cores.

There are two main types of parallelism:
- **Task-level parallelism** – performing _different operations_ (tasks) on the same or different data sets simultaneously.
- **Data-level parallelism** – performing _the same operation_ on different subsets of data simultaneously (distributing the workload across multiple workers).

**Heterogeneous computing** refers to systems that use more than one type of processor or processing core to maximize performance or energy efficiency, typically combining CPUs with specific accelerators like GPUs, TPUs, or FPGAs.

## Creating thread 
The application entry point, `{cpp}main()`, executes in the main thread. Any function called directly from the main thread also runs within it. Creating a new thread is referred to as _launching a thread_.

To launch a thread, we create a `std::thread` object (from `<thread>`). Execution begins _immediately_ upon construction, running the _callable object_ (function, lambda, or functor) provided as an argument.
>[!code-ref] `std::thread`
>```cpp
>template<class F, class... Args>
>explicit thread(F&& f, Args&&... args);
>```

Any _properly constructed_ (which takes as constructor argument callable object) thread object represent an active thread of execution in hardware level. Such a thread object is _joinable_. For any joinable thread we must call either `{cpp}join()` or `{cpp} detach()` function, which switch thread to _non-joinable_ state. If thread object is still joinable and for such object called destructor, `std::terminate` function will be called, which makes program an _unsafe program_. A thread object crated with default constructor doesn't represent a thread, thus it's non-joinable.  

For any joinable thread, we **must** call either `.join()` or `.detach()` to transition the object to a _non-joinable_ state before its destructor runs. If the destructor of a joinable thread object is invoked (e.g., it goes out of scope), `std::terminate()` is called, resulting in _abnormal program termination_.
The `.join()` member function introduces a _synchronization point_ between the launched thread and the thread it was launched from, blocking execution until the launched thread finishes. 
The `.detach()` function _separates_ the launched thread from the thread object, allowing execution to continue independently; allocated resources are freed automatically upon completion. 
The `.joinable()` boolean member function can be used to verify the current state of the thread object.

n most scenarios, between launching thread and joining thread we want to perform some other processing (continue with sequential execution in main thread, creating another thread etc). In this in-between section an exception can be thrown, which is dangerous since execution flow will not reach synchronization point (call to a `.join()` function).

One of solutions is using `try-catch` blocks, but this forces us to add `.join()` in both blocks, inside `try` and `catch`, which doesn't follow DRY principle. Instead, we should use _RAII_. We can create a _thread_guard_ class which takes in constructor created thread and in destructor calls `.join()`. A thread_guard doesn't take ownership of thread object, instead it stores a reference.
```cpp
class thread_guard {
    std::thread& m_thread;
public:
    // Important to make constructor explicit to prevent from any possible implicit conversion
    explicit thread_guard(std::thread& thread) : m_thread{thread} {}
    ~thread_guard() {
        if (m_thread.joinable())
            m_thread.join();
    }
    // Disable copy and move semantics
    thread_guard(const thread_guard&) = delete;
    thread_guard& operator=(const thread_guard&) = delete;
    thread_guard(thread_guard&&) = delete;
    thread_guard& operator=(thread_guard&&) = delete;
};
```

Alternatively, we can create _scoped_thread_ class which takes ownership over thread object using move semantics (which then will use`{cpp}thread(thread&& other) noexcept;` constructor) 
```cpp
class scoped_thread {
    std::thread m_thread;
public:
    explicit scoped_thread(std::thread&& thread) : m_thread{std::move(thread)} {}
    ~scoped_thread() {
        if (m_thread.joinable())
            m_thread.join();
    }
    // Based on Rule of Five, since we defined destructor - move semantics disabled by compiler and copy semantics disabled since class members not copyable
    // yet, it's better to implicitly delete move and copy special member functions
};
```
Staring from C++20, STL provide `std::jthread` objects. They almost similar to `std::thread`, except that`jthread` automatically rejoins on destruction.

Every active thread has a unique identifier Thread ID (TID), which can be retrieved using `{cpp}std::this_thread::get_id()`. This function returns a `std::thread::id` object. While it supports output via `{cpp}operator<<`, `std::thread::id` is primarily designed to serve as a key in associative containers (both ordered and unordered).
```cpp
void foo() {
    std::cout << "foo() [thread ID " << std::this_thread::get_id() << "]\n";
}

struct Functor {
    void operator()() const {
        std::cout << "Functor [thread ID " << std::this_thread::get_id() << "]\n";
    }
};

int main() {
    const std::thread::id tid = std::this_thread::get_id();
    std::cout << "main() [thread ID " << tid << "]\n";

    std::thread t1 {foo};
    thread_guard t1guard {t1};

    std::thread t2 {[]{
        using namespace std::this_thread;
        std::cout << "lambda [thread ID " << get_id() << "]\n";
    }};
    scoped_thread st2 {std::move(t2)};

    std::jthread t3 {Functor{}};

    return 0;
}
```

By default, the `std::thread` constructor copies supplied arguments into internal storage (using _decay-copy_), where they can be accessed by the new thread of execution. Even if the thread's function expects a reference, the constructor will blindly copy the value unless explicitly wrapped in `std::ref`. 
```cpp
void foo(int& a) { std::cout << "foo() " << a << "\n"; }

void bar(int a) { std::cout << "bar() " << a << "\n"; }

int main() {
    int val {3};
    std::jthread t1 {foo, std::ref(val)};
    std::jthread t2 {bar, val};
}
```

A critical issue arises when passing pointers or references to local variables into a thread that is lately detached.
If the thread is detached, the parent thread may finish execution and exit the scope (destroying local variables) before the child thread accesses those variables. This leads to _Undefined Behavior (UB)_: the child thread attempts to access a memory location that has been released (dangling reference), which in some cases can throw an exception. 
```cpp
void foo(int& i) {
    i += 1; // Potential access to destroyed variable
}

void bar() {
    int local_state = 0;
    std::thread t(func, std::ref(local_state));
    
    t.detach(); 
    // 'local_state' is destroyed here. 
    // If 't' runs after this line, it accesses invalid memory.
}
```
Solutions are passing variable by value (create copy), instead by reference or replace reference with `std::shared_ptr` to keep the data alive as long as at least one thread references it.

Another dangerous scenario involves implicit type conversion of arguments. The conversion might happen _in the context of the new thread_, creating a race condition regarding the lifespan of the source variable.
```cpp
void foo(std::string& s);

void bar(int param) {
    char buffer[1024];
    sprintf(buffer, "%d", param);

    /* DANGER: The std::thread constructor copies the 'char*' pointer, NOT the string.
     The conversion to std::string happens inside the new thread.
     'bar' might return and destroy 'buffer' BEFORE conversion completes. */
    //std::thread t(func, buffer); 
    
    std::thread t(func, std::string(buffer)); // Safe: string is constructed immediately
    t.detach();
}
```

## Some useful operations on thread 
We have already encountered the non-member function `{cpp}std::this_thread::get_id()`, which returns the unique identifier of the _current_ thread.
The `std::thread` object also has a member function `{cpp}.get_id()`.
- If the thread object represents an active thread of execution, it returns the unique TID.
- If the thread object is not active (e.g., default constructed, moved from, or already joined/detached), it returns a default-constructed `std::thread::id` (often represented as 0 in output), indicating "no thread".

`{cpp}void std::this_thread::sleep_for(const std::chrono::duration<Rep, Period>& sleep_duration);` - Blocks the execution of the current thread for _at least_ the specified duration `sleep_duration`. This function may block for longer period due to scheduling or resource contention delays. 
`{cpp}void std::this_thread::sleep_until(const std::chrono::time_point<Clock, Duration>& sleep_time);` - Blocks execution until a specific point in time in the future. If `std::sleep_for` we can compare to _timer_, `std::sleep_unit` will be _alarm clock_ in such analogy. 

`{cpp}void std::this_thread::yield()` provides a hint to the implementation to reschedule the execution of threads, allowing other threads to run. The amount of time that expires until the thread will be executed again is usually entirely dependent upon the scheduler. 

`{cpp}static unsigned int std::this_thread::hardware_concurrency()` - Returns an estimate of the number of concurrent threads supported by the implementation (typically matches the number of CPU cores). This value is a _hint_ (returns 0 if not computable). Creating significantly more threads than available hardware cores leads to excessive context switching, degrading performance, so it's wise to create threads amount less than that hint value. 

## Parallel accumulate algorithm using std::threads
Let's say we have an array with a large number of elements. If we want to calculate the sum of all elements, a standard approach iterates over the container and updates the result value sequentially. Since the summation operation is generally independent for each element, this calculation can be executed in parallel using _data-level parallelism_: we split the input iterator range into blocks and launch threads (workers) over each block of data.

With a small container size, the overhead of parallelization is not worth the cost. Therefore, we firstly define a `min_block_size`. By dividing the container size by this minimal block size (using ceiling rounding logic), we obtain the required number of threads, which _includes the main thread_. Since the physical number of threads is limited, we adjust this count based on `{cpp}std::thread::hardware_concurrency()`. This function can return 0, so this case must be handled explicitly. It is obvious that if only 1 thread is required, we already have it (the main thread). We can handle this "zero or one" case in a single conditional check `{cpp}if (numThread < 2)` as an early exit. This prevents us from spending resources on temporary vectors that won't be used when all calculations are performed in the main thread. We also need to recalculate the block size because the number of threads might have decreased after adjusting for hardware limits.

As mentioned, the calculated thread count includes the main thread, meaning the number of `std::thread` objects to launch will be one less. We need a temporary vector for these thread objects and another vector to store the calculation results of all threads, including the main one. While we could use the `init` value to store the main thread's result directly, this might break commutativity. For simple number summation, it is acceptable, but for types like strings, order matters, so storing all partial results in a vector is safer.

To launch `std::thread` objects, it is simplest to use a lambda function capturing the results vector by reference; other values should ideally be passed by copy. Using a helper iterator and `std::advance`, we calculate the block end, pass the iterator range (current begin to block end) to the thread, and then advance the begin iterator for the next block.

Finally, we must explicitly join the created workers. Since we need to accumulate their partial results into the final sum, we cannot rely solely on the destructors of `std::jthread` or other scope-based guards. The final accumulation happens within the current scope, and attempting to read results while workers are still running would lead to a race condition. Therefore, an explicit wait loop is mandatory before the final reduction step.


```cpp
static constexpr auto min_block_size = 1000;

template<typename Iterator, typename T>
T parallel_accumulate(Iterator begin, Iterator end, T init) {
    const auto inputSize = std::distance(begin, end);

    if (inputSize == 0)
        return init;

    // Calculate max threads based on data size
    const auto allowedThreads = static_cast<unsigned>((inputSize + min_block_size - 1) / min_block_size);
    // Determine actual number of threads (workers + main)
    const auto numThreads = std::min(std::thread::hardware_concurrency(), allowedThreads);

    // If only 1 thread is needed, avoid overhead and run on main thread
    if (numThreads < 2) {
        return std::accumulate(begin, end, init);
    }
    
    // amount of thread require beside main thread 
    const auto numWorkers = numThreads - 1;

    std::vector<std::thread> threads(numWorkers);
    std::vector<T> results(numThreads); // Store results for all chunks

    // Calculate block size for workers (integer division truncates)
    const auto block_size = inputSize / numThreads;

    // Launch worker threads
    Iterator blockStart = begin;
    for (size_t i = 0; i < numWorkers; ++i) {
        Iterator blockEnd = blockStart;
        std::advance(blockEnd, block_size);

        threads[i] = std::thread([blockStart, blockEnd, i, &results] {
            results[i] = std::accumulate(blockStart, blockEnd, T{});
        });

        blockStart = blockEnd;
    }

    // Process the final chunk from current position to the end. Main function already use one thread, so
    // creating additional thread for final chunk will be redundant
    results.at(numWorkers) = std::accumulate(blockStart, end, T{});

    // Wait until all workers finish calculation
    std::ranges::for_each(threads, std::mem_fn(&std::thread::join));

    return std::accumulate(results.begin(), results.end(), init);
}

int main() {
    std::vector<int> input(8500);
    std::mt19937 rEng{std::random_device{}()};
    std::uniform_int_distribution idist(0, 10);

    std::ranges::generate(input, [&] { return idist(rEng); });

    std::cout << parallel_accumulate(input.begin(), input.end(), 0) << '\n';
    return 0;
}
```

## thread_local variables
When we declare a variable with the `thread_local` keyword, each thread receives its own distinct copy of that variable. The storage duration spans the entire execution of the thread in which it was created, and the value is initialized when the thread starts (more precisely, on first use within that thread).

### More details from claude
The `thread_local` specifier can be applied to variables at different scopes: with global variables, namespace-scope variables, static class members, and local static variables. We cannot apply `thread_local` to non-static class members, automatic local variables, or function parameters. The storage model simply doesn't make sense for these cases since they already have their own per-invocation or per-object semantics.

Thread-local storage becomes valuable in several scenarios. The most common is eliminating contention when we need per-thread state that would otherwise require synchronization. For example, a random number generator if every thread shares one generator protected by a mutex, we create a serialization bottleneck. With `thread_local std::mt19937 rng;`, each thread gets its own generator with no locking needed.
Another strong use case is accumulating thread-local results that we later aggregate. Rather than having all threads increment a shared atomic counter (causing cache line bouncing), each thread can increment its own `thread_local` counter, and we sum them up at the end. 
Note that main thread is also thread, meaning that it will have own copy of `thread_local` variable: for mentioned above example we have different solutions:
* Store pointer to each `thread_local` variable 
```cpp
std::mutex registration_mutex;
std::vector<int*> all_counters;

thread_local int counter = 0;

void register_thread_local_counter() {
    std::lock_guard<std::mutex> lock(registration_mutex);
    all_counters.push_back(&counter);
}

void worker_thread() {
    register_thread_local_counter();
	counter++; 
}
```
* Return `thread_local` variable 
```cpp
thread_local int counter = 0;

int worker_thread() {
    counter++;
    return counter;
}

int main() {
    std::vector<std::future<int>> futures;
    for (int i = 0; i < 10; ++i)
        futures.push_back(std::async(std::launch::async, worker_thread));
}
```
* Use thread-safe container 
```cpp

std::mutex results_mutex;
std::map<std::thread::id, int> thread_results;

thread_local int counter = 0;

void worker_thread() {
    counter++;  
    {
        std::lock_guard<std::mutex> lock(results_mutex);
        thread_results[std::this_thread::get_id()] = counter;
    }
}
```
And it can be faster than using atomic variables, mostly due to constant competition for cache line.

`thread_local` increases memory usage proportionally to thread count. If we have a `thread_local` array of 1MB and spawn 100 threads, resulting memory usage will be 100MB. Also, `thread_local` variables don't automatically aggregate or synchronize. If we need final combined results, we must explicitly iterate through thread-local values (which requires keeping track of them, perhaps in a thread-safe container) or use a different approach. The destructor ordering between different `thread_local` variables within a thread follows the reverse order of their construction, similar to regular static variables but scoped to the thread's lifetime.