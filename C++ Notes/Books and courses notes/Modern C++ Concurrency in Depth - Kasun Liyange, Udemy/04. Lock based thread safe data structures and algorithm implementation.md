When developing concurrent application, one of our main concerns are
+ Invariant should not become broken.
+ Avoiding race conditions, include inherited from  interface
+ Handling exception scenarios. 
+ Avoiding deadlocks. 

## Thread safe queue implementation 
> This is mostly re-explanation of 7.2.6 Writing a thread-safe queue without locks from book "C++ Concurrency in Action" (second edition) by Anthony Williams

As discussed in [[03. Communication between threads using condition variables and futures#Thread safe queue implementation|previous example]], using a single mutex limits concurrency: `{cpp}push()` and `{cpp}pop()` operations are limited to one thread access only through single mutex for both operations, even though accessing the head and the tail involves different parts of the data structure. To improve performance, we need to separate the locking logic for the head and the tail.

A standard queue is implemented as a singly linked list. Each node contains data and a pointer to the next node. We use `std::unique_ptr` for the `nextPtr` to manage the lifecycle of the nodes automatically. Regarding data storage, as discussed previously, we use `std::shared_ptr<T>`. This ensures exception safety (making `pop` `noexcept` by returning a pointer) and allows safe data sharing.

Using `std::unique_ptr<T>` for data seems lighter. However, creating a `std::shared_ptr` inside `{cpp}pop()` (to return it safely) would require memory allocation while holding the lock. By allocating the `std::shared_ptr` inside `{cpp}push()` (outside the lock), we move the expensive allocation overhead out of the critical path of the consumer threads.
```cpp
struct node {
	std::shared_ptr<T> dataPtr {nullptr};
    std::unique_ptr<node> nextPtr {nullptr};
};
```

In queue we have two pointers - pointer to tail and pointer to head. Head we already discussed, is actual owner of whole linked list. While tail - is more a observer pointer - it doesn't own anything and pointing to `nextPtr` of last created node.

Separating mutexes introduces a specific **race condition** when the queue is empty (or nearly empty). If `head == tail`, both `{cpp}push()` and `{cpp}pop()` threads might try to modify the same node pointers simultaneously using different mutexes, leading to data corruption or undefined behavior.
To solve this, we use a *dummy node* (sentinel node) at the tail. This design maintains an invariant where the queue always has at least one node. The `m_head` pointer addresses the first real node (or the dummy if empty), while `m_tailObserver` always points to the dummy node itself. This separation ensures that `{cpp}push()` accesses `m_tailObserver` (protected by the tail mutex), while `{cpp}pop()` accesses `m_head` (protected by the head mutex). Consequently, threads never contend for the same pointer fields unless the logic explicitly requires checking the queue's empty state.

Since dummy node actually will be in the tail, it's safe to use raw pointer to tail. When we pushing new value, we create new dummy node, using pointer to old dummy (old tail) we update `dataPtr` with new data, move dummy node in `nextPtr` and update tail pointer to new dummy node. 

Head node we already discussed that it's going be actual owner of lined listed. We store that node in queue class, and went want to pop node from head - we move current head to scope-local `std::unique_ptr`, move it's `nextPtr` into head's `std::unique_ptr`, take from poped node `std::shared_ptr` to data and safely return data to caller. 

We employ two mutexes: `m_headMutex` and `m_tailMutex`. It is critical to keep the critical sections as small as possible, so expensive operations like memory allocation (`make_shared`, `make_unique`) are performed outside the locks. The queue is considered empty when `{cpp}m_head.get() == get_tail()`. This check requires acquiring `m_tailMutex` briefly to read the tail pointer safely.
```cpp
template<typename T>
class sequential_queue {
    struct node {
        std::shared_ptr<T> dataPtr;
        std::unique_ptr<node> nextPtr;
    };

public:
    sequential_queue() : m_head(std::make_unique<node>()), m_tailObserver(m_head.get()) {}

    void push(T value) {
        // Create dummy node data value in advance
        auto const newDataPtr {std::make_shared<T>(std::move(value))};
        auto dummy {std::make_unique<node>()};
        node* const new_tail{dummy.get()};

        { // Limiting critical sector
            std::lock_guard tail_lock(m_tailMutex);

            m_tailObserver->dataPtr = newDataPtr;
            m_tailObserver->nextPtr = std::move(dummy);
            m_tailObserver = new_tail;
        }

        m_cv.notify_one();
    }

    std::shared_ptr<T> pop() {
        std::lock_guard headLock(m_headMutex);
        if (m_head.get() == get_tail())
            return {};

        return pop_head_node();
    }

    std::shared_ptr<T> wait_pop() {
        std::unique_lock headLock(m_headMutex);
        m_cv.wait(headLock, [&]{ return m_head.get() != get_tail(); });

        return pop_head_node();
    }

private:
    // Expected that function called under mutex
    std::shared_ptr<T> pop_head_node() {
        std::unique_ptr<node> oldHead = std::move(m_head);
        m_head = std::move(oldHead->nextPtr);
        return oldHead->dataPtr;
    }

    node* get_tail() {
        std::lock_guard tailLock(m_tailMutex);
        return m_tailObserver;
    }

    std::unique_ptr<node> m_head;
    node* m_tailObserver;

    std::mutex m_headMutex;
    std::mutex m_tailMutex;
    std::condition_variable m_cv;
};
```

## Parallel STL 
Starting from C++17, STL allows developers to specify an _execution policy_ for algorithms, indicating a preference for parallel or sequential execution. It is important to note that the policy is a hint; the implementation (compiler/library) decides whether to actually run it in parallel based on resources.

In most cases, parallel algorithms are identical to their sequential counterparts, accepting an execution policy as the first argument. However, there are exceptions, like `{cpp}std::accumulate` has no execution policy overload and  for parallel summation, `{cpp}std::reduce` must be used. The `std::ranges` library currently does not support parallel execution policies.

Most parallel algorithm overloads require at least a **Forward Iterator**. This limits the types of data sources that can be used. For example sequential `{cpp}std::find_if()` require input iterator, meaning that `std::stringsteam` can be used only with sequential overload. Fortunately, standard containers (`std::vector`, `std::list`, etc.) satisfy this requirement.

When using standard execution policy and an exception is thrown, `std::terminate` will be called, while for sequential execution we can wrap algorithm execution with try-catch block. 

There are 4 execution policies in the STL library:
1. **`std::execution::seq`** Used to force sequential execution in the current thread. It requires no synchronization mechanisms but prevents compiler ambiguity when selecting overloads.
2. **`std::execution::par`** Execution _may_ be parallelized across multiple threads. Inside each thread operations are ordered sequentially, use of mutexes or atomics is safe.
3. **`std::execution::par_unseq`** Execution _may_ be parallelized across threads and vectorized. Inside each thread Operations may be interleaved or unordered (SIMD), use blocking synchronization (mutexes) can cause deadlocks.
4. **`std::execution::unseq`** _(C++20)_ Execution is on a single thread but _may_ be vectorized. Uses SIMD instructions to operate on multiple data items at once, synchronization (mutexes) is not allowed due to instruction interleaving.

** **SIMD** (Single Instruction, Multiple Data) is a hardware capability allowing a single CPU core to perform the same operation on multiple data points simultaneously (e.g., using AVX-512 extensions).

```cpp
constexpr size_t DATA_SIZE = 100'000'000;

double heavy_math(const double v) {
    return std::sqrt(std::abs(v * std::sin(v)));
}

template <typename Policy>
void run_benchmark(const std::string& name, Policy policy, const std::vector<double>& src, std::vector<double>& dst) {
    using namespace std::chrono;

    auto start = high_resolution_clock::now();
    std::transform(policy, src.begin(), src.end(), dst.begin(), [](const double v) {
        return heavy_math(v);
    });
    auto end = high_resolution_clock::now();

    std::println("{}: {} ms", name, duration_cast<microseconds>(end - start).count());
    
    if (dst.empty()) 
	    std::cerr << "Error";
}

int main() {    
    std::vector<double> src(DATA_SIZE);
    std::vector<double> dst(DATA_SIZE);

    std::mt19937 gen(std::random_device{}());
    std::uniform_real_distribution dis(0.1, 100.0);
    std::generate(std::execution::par, src.begin(), src.end(), [&] { return dis(gen); });

    std::println("Starting benchmarks...");

    run_benchmark("std::execution::seq", std::execution::seq, src, dst);
    run_benchmark("std::execution::unseq", std::execution::unseq, src, dst);
    run_benchmark("std::execution::par", std::execution::par, src, dst);
    run_benchmark("std::execution::par_unseq", std::execution::par_unseq, src, dst);

    return 0;
}
/* Possible output 
std::execution::seq: 1236337 ms
std::execution::unseq: 1222246 ms
std::execution::par: 116055 ms
std::execution::par_unseq: 116185 ms
*/
```

## Parallel quick sort algorithm 
Quick sort is one of most used sorting algorithm due to it's efficiency with average complexity in sequential execution $O (n * log n)$
Algorithm based on 4 basic steps: 
1. Select a pivot element;
2. Reorder elements in such way, that elements lower that pivot will be moved before pivot and greater elements after the pivot. This will create two ranges;
3. Recursively call to same sort function in each of two half; 
4. Arrange the results of recursive calls

```cpp
template<typename T>
std::list<T> sequential_quick_sort(std::list<T> input) {
    // Recursive return condition
    if (input.size() < 2)
        return input;

    // Splice will transfer from input list only first element. We want to separate our future pivot to avoid compare
    // pivot with itself
    std::list<T> pivotList;
    pivotList.splice(pivotList.begin(), input, input.begin());
    const T& pivot = pivotList.front();

    // Rearrange based on pivot
    auto divPoint = std::partition(input.begin(), input.end(), [&](const T& t) { return t < pivot; });
    
    // splice function will efficiently transfer from input list to lowerList up to divPoint,
    // so in input array will only left element after divPoint
    std::list<T> lowerList;
    lowerList.splice(lowerList.end(), input, input.begin(), divPoint);

    // Recursive call to lower side, result at the end will contain sorted half 
    auto result = sequential_quick_sort(std::move(lowerList));
    
    // Return isolated pivot back to result list
    result.splice(result.end(), pivotList);
    
    // Recursive call to upper side. Result then transferring to the end, after pivot
    auto newUpperList = sequential_quick_sort(std::move(input));
    result.splice(result.end(), newUpperList);

    return result;
}
```

Making this algorithm parallel is quite simple - we replace recursive call to parallel task using `std::future`
```cpp
template<typename T>
std::list<T> parallel_quick_sort(std::list<T> input) {
    // Recursive return condition
    if (input.size() < 2)
        return input;

    std::list<T> pivotList;
    pivotList.splice(pivotList.begin(), input, input.begin());
    const T& pivot = pivotList.front();

    auto divPoint = std::partition(input.begin(), input.end(), [&](const T& t) { return t < pivot; });

    std::list<T> lowerList;
    lowerList.splice(lowerList.end(), input, input.begin(), divPoint);

    std::future<std::list<T>> newUpperList = std::async(std::launch::async,
                                                        &parallel_quick_sort<T>,
                                                        std::move(input));
    auto result = parallel_quick_sort(std::move(lowerList));

    result.splice(result.end(), pivotList);
    result.splice(result.end(), newUpperList.get());

    return result;
}
```

>[!note]
> 1. This implementation doesn't take into account exponential threads growth, which result to termination from OC in some cases (`Resource temporarily unavailable`)
> ```cpp fold="Possible implementation"
>template<typename T>
>std::list<T> parallel_quick_sort(std::list<T> input, unsigned depth) {
>    if (input.size() < 2)
>        return input;
>
>    std::list<T> pivotList;
>    pivotList.splice(pivotList.begin(), input, input.begin());
>    const T& pivot = pivotList.front();
>
>    auto divPoint = std::partition(input.begin(), input.end(),
>                                   [&](const T& t) { return t < pivot; });
>
>    std::list<T> lowerList;
>    lowerList.splice(lowerList.end(), input, input.begin(), divPoint);
>
>    std::packaged_task upperTask(&parallel_quick_sort<T>);
>    auto upperFuture = upperTask.get_future();
>
>    std::optional<std::thread> upperThread;
>    unsigned next_depth = depth > 0 ? depth - 1 : 0;
>    if (depth > 0)
>        upperThread.emplace(std::move(upperTask), std::move(input), next_depth);
>    else
>        upperTask(std::move(input), next_depth);
>
>    auto result = parallel_quick_sort(std::move(lowerList), next_depth);
>
>    if (upperThread)
>        upperThread->join();
>
>    result.splice(result.end(), pivotList);
>    result.splice(result.end(), upperFuture.get());
>
>    return result;
>}
> ```
> 2. Even thought quick sort is quicker than `std::list`member function `sort()`, it's still much slower than using temporary vector and `std::sort` even with `std::execution::seq` and time expenses on crate temporary vector and create result list from sorted vector. 
> ```
> // 50'000'000 elements 
> Member function sort 21820ms
> Seq quick sort 7492ms
>Parallel quick sort 3387ms
>Temp vector sort 4652ms
>Parallel Temp vector sort 1747ms
> ```

## Parallel for each implementation 
General idea is similar to examples from [[01. Thread management guide#Parallel accumulate algorithm using std threads|Parallel accumulate algorithm using std threads]] and [[03. Communication between threads using condition variables and futures#Parallel accumulate algorithm implementation with std async task|Parallel accumulate algorithm implementation with std::async_task]], since generally speaking, `{cpp}std::accumulate()` is same as `{cpp}std::for_each(res += n)`. 

One approach with `std::thread` and `std::packaged_task` is divide iterator range on blocks and execute `{cpp}std::for_each()` and separate thread with each own block.
```cpp
template<typename Iter, typename Func>
void parallel_for_each_pt(Iter begin, Iter end, Func func) {
    const auto length = std::distance(first, end);
    if (!length) return;

    constexpr auto minPerThread {2000U};
    const auto allowedThreads = static_cast<unsigned>((length + minPerThread - 1) / minPerThread);
    const auto hwCon {std::thread::hardware_concurrency()};
    const auto numThreads {std::min(hwCon != 0 ? hwCon : 2U, allowedThreads)};
    const auto blockSize {length / numThreads};

    std::vector<std::future<void>> futures(numThreads - 1);
    std::vector<std::jthread> threads(numThreads - 1);

    Iter blockStart {begin};
    for (auto i = 0U; i <= numThreads - 2; ++i) {
        Iter blockEnd {blockStart};
        std::advance(blockEnd, blockSize);
        std::packaged_task<void()> task {[=]{std::for_each(blockStart, blockEnd, func);}};
        futures[i] = task.get_future();
        threads[i] = std::jthread(std::move(task));
        blockStart = blockEnd;
    }

    std::for_each(blockStart, end, func);

    for (auto& future : futures)
        future.get();
}
```

For `std::future` and `std::async` - split iterator range on half and recursively call our `{cpp}for_each` implementation with each half in separate thread. 
> However, provided by course author example doesn't take in account exponential threads growth, which cause almost x4 slower implementation with `std::async` and even can cause program termination by OS. I tried to modify that version with help of AI.

```cpp
template<typename Iter, typename Func>
void parallel_for_each_async(Iter begin, Iter end, Func func, unsigned depth = 0) {
    const auto length = std::distance(begin, end);
    if (!length) return;

    const auto maxDepth = std::thread::hardware_concurrency();
    constexpr auto minPerThread {2000U};

    if (depth >= maxDepth || length < minPerThread) {
        std::for_each(begin, end, func);
        return;
    }

    Iter const midPoint = begin + length / 2;
    auto firstHalfFuture = std::async(std::launch::async,
                                       &parallel_for_each_async<Iterator, Func>,
                                       begin, midPoint, func, depth + 1);
    parallel_for_each_async(midPoint, end, func, depth + 1);
    firstHalfFuture.get();
}
```

Since C++17 STL provide execution policy for `{cpp}std::for_each()`, which shows better results. Probably better to relay on STL developers in this case. 
```
Parallel for each with std::packaged_task 1553ms
Parallel for each with std::async 1281ms
seq std::for_each 19564ms
par std::for_each 1249ms
```

## Parallel find algorithm 
Main challenge of find algorithm - we need to stop algorithm when value found.
```cpp
template<typename Iter, typename T>
Iter parallel_find_pt(Iter begin, Iter end, T match) {
    const auto length = std::distance(begin, end);
    if (!length) return end;

    constexpr auto minPerThread {2000U};
    const auto allowedThreads = static_cast<unsigned>((length + minPerThread - 1) / minPerThread);
    const auto hwCon {std::thread::hardware_concurrency()};
    const auto numThreads {std::min(hwCon != 0 ? hwCon : 2U, allowedThreads)};
    const auto blockSize {length / numThreads};

    struct find_element {
        void operator()(Iter begin, Iter end, T match, std::promise<Iter>* result, std::atomic<bool>* found) {
            try {
                for (auto it = begin; it != end && !found->load(std::memory_order_relaxed); ++it) {
                    if (*it != match)
                        continue;
                    found->store(true, std::memory_order_relaxed);
                    result->set_value(it);
                    return;
                }
            }
            catch (...) {
                found->store(true, std::memory_order_relaxed);
                result->set_exception(std::current_exception());
            }
        }
    };

    std::vector<std::jthread> threads(numThreads - 1);
    std::promise<Iter> promise;
    std::atomic foundFlag {false};

    Iter blockStart {begin};
    for (auto i = 0U; i <= numThreads - 2; ++i) {
        Iter blockEnd {blockStart};
        std::advance(blockEnd, blockSize);

        threads[i] = std::jthread(find_element{}, blockStart, blockEnd, match, &promise, &foundFlag);
        blockStart = blockEnd;
    }

    find_element{}(blockStart, end, match, &promise, &foundFlag);

    return foundFlag.load(std::memory_order_acquire) ? promise.get_future().get() : end;
}
```

> [!note] 
> But in this authors' example can be race condition - if two threads will find element - they can both change `std::promise`, which can lead to crash, since `std::promise` can be used only once. If guard `std::promise` with mutex, it's become quite questionable design - atomic, mutex and shared state together can create redundancy. 
>```cpp fold="Probably improved version"
>template<typename Iter, typename T>
>Iter parallel_find_pt(Iter begin, Iter end, T match) {
>    const auto length = std::distance(begin, end);
>    if (!length) return end;
>
>    constexpr auto minPerThread {2000U};
>    const auto allowedThreads = static_cast<unsigned>((length + minPerThread - 1) / minPerThread);
>    const auto hwCon {std::thread::hardware_concurrency()};
>    const auto numThreads {std::min(hwCon != 0 ? hwCon : 2U, allowedThreads)};
>    const auto blockSize {length / numThreads};
>
>    std::vector<std::jthread> threads(numThreads - 1);
>    std::promise<Iter> promise;
>    std::atomic foundFlag {false};
>
>    auto find_element = [&](Iter blockBegin, Iter blockEnd) {
>        try {
>            for (auto it = blockBegin; it != blockEnd && !foundFlag.load(std::memory_order_relaxed); ++it) {
>                if (*it != match) continue;
>                if (bool expected = false
>                    ; foundFlag.compare_exchange_strong(expected, true, std::memory_order_acq_rel)) {
>                    promise.set_value(it);
>                }
>                return;
>            }
>        }
>        catch (...) {
>            if (bool expected = false
>                ; foundFlag.compare_exchange_strong(expected, true, std::memory_order_acq_rel)) {
>                promise.set_exception(std::current_exception());
>            }
>        }
>    };
>
>    Iter blockStart {begin};
>    for (auto i = 0U; i <= numThreads - 2; ++i) {
>        Iter blockEnd {blockStart};
>        std::advance(blockEnd, blockSize);
>
>        threads[i] = std::jthread(find_element, blockStart, blockEnd);
>        blockStart = blockEnd;
>    }
>
>    find_element(blockStart, end);
>
>    return foundFlag.load(std::memory_order_acquire) ? promise.get_future().get() : end;
>}
>```  
> Again, STL algorithm outperform, also with this algorithm instead first found element we actually return from random position matching element   
> ```
>seq std::find 301ns (distance 112)
>par std::find 47855ns (distance 112)
>Parallel find with blocks and thread 236148ns (distance 50000139)
> ```

Implementation with recursive call and `std::async`. Compare to implementation with `std::thread`, in this implementation we can also provide similarly to `{cpp}std::find()` position of first match, but interface require external `foundFlag`, so probably also require another template which will call actual implementation.
```cpp
template<typename Iter, typename T>
Iter parallel_find_async(Iter begin, Iter end, T match, std::atomic<bool>* foundFlag, unsigned depth = 0){
    try {
        auto length = std::distance(begin, end);
        if (!length) return end;

        const auto maxDepth = std::log2(std::thread::hardware_concurrency());
        constexpr auto minPerThread {2000U};

        if (depth >= maxDepth || length < minPerThread) {
            for (auto it = begin; it != end && !foundFlag->load(std::memory_order_relaxed); ++it) {
                if (*it == match) {
                    *foundFlag = true;
                    return it;
                }
            }
            return end;
        }

        Iter const midPoint = begin + length / 2;
        auto futureRes = std::async(std::launch::async,
            [=] {
                return parallel_find_async(midPoint, end, match, foundFlag, depth + 1);
            });
        Iter res1 = parallel_find_async(begin, midPoint, match, foundFlag, depth + 1);
        Iter res2 = futureRes.get();

        if (res1 != midPoint) return res1;
        return res2;
    }
    catch (const std::exception&) {
        foundFlag->store(true);
        throw;
    }
}
```
