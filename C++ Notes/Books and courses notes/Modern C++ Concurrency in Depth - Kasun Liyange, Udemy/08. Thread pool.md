On most systems, it's impractical to create separate thread for every parallel tasks. Thread pools allows to manage tasks and existing threads by introducing  tasks queue and threads-workers which takes task from queue and execute in the loop. When task is executed by worker - task's result will be accessible via `std::future`.

For simple implementation we require any thread safe queue. Also, thread pool should support "wait" functionality when current tasks gets finished and when queue is not empty. 

Since for `std::future` requires to specify return type, in `submit` function we can use some meta-programming magic `{cpp}std::invoke_result_t<std::decay_t<Func_t>>`. We also need to use [[03. Communication between threads using condition variables and futures#std package_task|packaged task]] to obtain `std::future` from callable object. 

Note that `std::package_task` can only accept move-only objects. We can use wrapper-class, 
```cpp
class move_only_function {
    struct impl_base {
        virtual void call() = 0;
        virtual ~impl_base() = default;  // = default предпочтительнее
    };

    template<typename F>
    struct impl_type final : impl_base {
        F f;
        explicit impl_type(F&& f_) : f(std::move(f_)) {}
        void call() override { f(); }
    };

    std::unique_ptr<impl_base> impl;

public:
    move_only_function() = default;
        
    move_only_function(function_wrapper&&) noexcept = default;
    move_only_function& operator=(function_wrapper&&) noexcept = default;
    
    move_only_function(const function_wrapper&) = delete;
    move_only_function& operator=(const function_wrapper&) = delete;
    
    template<typename F>
    explicit move_only_function(F&& f) 
        : impl(std::make_unique<impl_type<std::decay_t<F>>>(std::forward<F>(f)))
    {}

    void operator()() { impl->call(); } 
};
```
however from C++23 available  `std::move_only_function`.

```cpp
class thread_pool {
    using task_t = std::move_only_function<void()>;

    threadsafe_queue<task_t> work_queue;
    std::vector<std::jthread> threads;

    // Flag to stop thread execution
    std::atomic_bool done{false};

    void worker_thread() {
        while (!done.load(std::memory_order_acquire)) {
            if (task_t task; work_queue.pop(task))
                task();
            else
                std::this_thread::yield();
        }
    }

public:
    thread_pool() {
	    // one thread probably already used by main thread
        const auto thread_count = std::thread::hardware_concurrency() - 1;

        threads.reserve(thread_count);

        try {
            for (auto i = 0U; i < thread_count; i++) {
                threads.emplace_back(&thread_pool::worker_thread, this);
            }
        } catch (...) {
            done.store(true, std::memory_order_release);
            throw;
        }
    }

    ~thread_pool() {
        done.store(true, std::memory_order_release);
    }

    template<typename Func_t>
    auto submit(Func_t f) {
        using return_t = std::invoke_result_t<std::decay_t<Func_t> >;

        std::packaged_task<return_t()> task(std::move(f));
        auto res_future = task.get_future();
        work_queue.push(std::move(task));
        return res_future;
    }
};
```

In this implementation exist issue with `std::this_thread::yield()` - this can reduce CPU performance when thread doesn't have task to work with. Also here used "Hard stop" - thread pool doesn't check queue size and when destruction called - can drop tasks pending in queue. 

When discussed recursive sorting algorithm (quicksort), we mentioned that even though we going to processed with each half on separate thread, we actually need to create only one thread for one half since second half can be proceed in current thread, otherwise we actually will use three threads and third thread going simply wait for result of two other threads. Similar situation can happen with thread pools. We can provide in thread pool interface which will allow execute some task in caller (main) thread while waiting results from threads managed by thread pool. Here, if queue is empty - from "helper" thread we don't want to lose control with `yield()`. 
```cpp
class thread_pool {
private:
	void worker_thread() {  
	    while (!done.load(std::memory_order_acquire)) {  
	        if (!try_run_pending_task())  
	            std::this_thread::yield();  
	    }  
	}
public: 
    bool try_run_pending_task() {
        if (task_t task; work_queue.pop(task)) {
            task();
            return true;
        }
        return false;
    }
};
```

With single queue can raise problem with increasing contention on the queue. With each new task in `{cpp}sumbit()` function we have to push from one threads elements to queue, while workers continually popping items of the queue from single shared queue (which can cause "cache ping-pong"). This issue can be solved with introduction for each worker local queue. 

Local cache can create situation with unbalanced workload, when one thread have lots of tasks in it's local queue, while other threads waiting with empty queues, so we need mechanism to steal work from local queues. For local thread queue we need use deque since going locally operate on front of container and steal from back of container. It's better to use _lock-free_ deque, since if use thread-safe (with mutexes) deque implementation  - stealing-thread going spend resources on mutex waiting, which makes mechanism less efficient. But for example can be used locking deque (and even both for local and global queues).

To each thread in pool assigned index, which used as index to thread's own local queue. When willing execute some task, first it checks for a task in local queue, then in global queue, and then tries to steal task from another thread.  

Important to track lifetime of threads and local queues: thread can't start work before creating local queue and thread should be destroyed before destruction of queue. Since threads start work right after creation and can wait on mutexes/be suspended - there high chances to access invalid memory. 

Another note about local queues - they filled when threads themselves create tasks. In algorithms examples which uses recursion - this can be a case, yet if task submitted from main thread - workers will  constantly failing to obtain task from local queue and take tasks from global queue.  
```cpp
template <typename T>
class stealing_queue
{
    std::deque<T> the_queue;
    mutable std::mutex the_mutex;

public:
    void push(T data) {
        std::lock_guard<std::mutex> lock(the_mutex);
        the_queue.push_front(std::move(data));
    }

    bool empty() const {
        std::lock_guard<std::mutex> lock(the_mutex);
        return the_queue.empty();
    }

    bool try_pop(T& res) {
        std::lock_guard<std::mutex> lock(the_mutex);
        if (the_queue.empty()) return false;

        res = std::move(the_queue.front());
        the_queue.pop_front();
        return true;
    }

    bool try_steal(T& res) {
        std::lock_guard<std::mutex> lock(the_mutex);
        if (the_queue.empty()) return false;

        res = std::move(the_queue.back());
        the_queue.pop_back();
        return true;
    }
};

using task_t = std::move_only_function<void()>;

class thread_pool {
    using local_queue_t = std::unique_ptr<stealing_queue<task_t>>;

    std::atomic<bool> done {false};
    std::vector<local_queue_t> local_queues;
    stealing_queue<task_t> global_work_queue;
    // important to destroy threads first
    std::vector<std::jthread> threads;

    static thread_local stealing_queue<task_t>* thread_local_queue;
    static thread_local size_t my_index;

    void worker_thread(const size_t index)
    {
        my_index = index;
        thread_local_queue = local_queues[index].get();

        while (!done.load(std::memory_order_acquire)) {
            if (!try_run_pending_task()) {
                std::this_thread::yield();
            }
        }
    }

    bool steal_from_others(task_t& task) {
        if (local_queues.empty()) return false;

        for (unsigned i = 0; i < local_queues.size(); ++i) {
            // calculating index of thread from which going try to steal work in round-way
            unsigned const index = (my_index + i + 1) % local_queues.size();

            // Also trying to not steal from ourselves
            if (index == my_index) continue;

            if (local_queues[index]->try_steal(task))
                return true;
        }
        return false;
    }

public:
    thread_pool() {
        unsigned const thread_count = std::thread::hardware_concurrency();

        local_queues.reserve(thread_count);
		threads.reserve(thread_count);
        
        for (auto i = 0U; i < thread_count; ++i) {
            local_queues.push_back(std::make_unique<stealing_queue<task_t>>());
            threads.emplace_back(&thread_pool::worker_thread, this, i);
        }
    }

    ~thread_pool() {
        done.store(true, std::memory_order_release);
    }

    bool try_run_pending_task() {
        task_t task;

        // pop from local queue first
        if (thread_local_queue && thread_local_queue->try_pop(task)) {
            task();
            return true;
        }

        // then try from global queue
        if (global_work_queue.try_pop(task)) {
            task();
            return true;
        }

        if (steal_from_others(task)) {
            task();
            return true;
        }
        return false;
    }

    template<typename F>
    auto submit(F&& f) {
        using return_t = std::invoke_result_t<std::decay_t<F>>;
        std::packaged_task<return_t()> task(std::forward<F>(f));
        auto res = task.get_future();

        // Checking, is task-submitter - worker from poll.
        if (thread_local_queue)
            thread_local_queue->push(std::move(task));
        else
            global_work_queue.push(std::move(task));
        return res;
    }
};

// Initialize static variables
thread_local stealing_queue<task_t>* thread_pool::thread_local_queue = nullptr;
thread_local size_t thread_pool::my_index = 0;
```

> Although initial motivation was to avoid cache ping-pong due to reading/writing from one shared queue, this implementation potentially doesn't solve that issue since storing pointer to vectors - mean that threads have to cache cache to find specific work. Better container might be contiguous ring-buffer  (also with aligning to cache line size to avoid false sharing). Also, `yeald()` not suitable for performance, and can be better to use atomic's `wait()`.  

